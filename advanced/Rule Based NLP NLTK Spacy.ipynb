{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Traditional (Rule-Based NLP using NLTK and Spacy)**"
      ],
      "metadata": {
        "id": "E-bZui4bEzna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2bysXLMHEkq6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization- The process of breaking a text down into tokens is called tokenization.**"
      ],
      "metadata": {
        "id": "2j7mkx9DE8NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Mary, don’t slap the green witch\"\n",
        "print([str(token) for token in nlp(text.lower())])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQfiJ-iDErDB",
        "outputId": "0e5bc76d-f8ba-4509-9aeb-d43a8ddde0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mary', ',', 'do', 'n’t', 'slap', 'the', 'green', 'witch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Mary, don’t slap the green witch\"\n",
        "word_tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHamtVCFFf6R",
        "outputId": "0d8c82d4-0ed1-416d-9d84-be231dedf16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary', ',', 'don', '’', 't', 'slap', 'the', 'green', 'witch']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing tweets using NLTK\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet=\"Snow White and the Seven Degrees #MakeAMovieCold@midnight: )\"\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet.lower()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPbEVtF6GxbB",
        "outputId": "0ddb2c21-df1c-48e5-ac0e-f984cd9f8404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **2. Stopword Removal-Stopwords such as articles and\n",
        "prepositions serve mostly a grammatical purpose, like filler holding the content words.**"
      ],
      "metadata": {
        "id": "8p2PzPVUH0T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "text = \" we will show how to remove stopwords using spacy library\"\n",
        "\n",
        "lst=[]\n",
        "for token in text.split():\n",
        "    if token.lower() not in stopwords:    #checking whether the word is not\n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYkb0MGcIAWi",
        "outputId": "b93e4b19-7816-43b6-90d8-d826bd990206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['remove', 'stopwords', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('english')\n",
        "text = \" we will show how to remove stopwords using spacy library\"\n",
        "\n",
        "lst=[]\n",
        "for token in text.split():\n",
        "    if token.lower() not in stopwords:    #checking whether the word is not\n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZmRFLziIrbD",
        "outputId": "77cb4676-af63-444b-e798-6e641663b14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['show', 'remove', 'stopwords', 'using', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Unigrams, Bigrams, Trigrams, …, N-grams**\n",
        "N grams are fixed length (n) consecutive token sequences occurring in the text. A bigram has two\n",
        "tokens, a unigram one. Generating n grams from a text is straightforward enough, but packages like spaCy and NLTK provide convenient methods."
      ],
      "metadata": {
        "id": "VAsFn4dUKV2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def n_grams(text, n):\n",
        "  return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
        "print(n_grams(cleaned, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbBCNzdnKBq_",
        "outputId": "c9c4392c-4b59-4ea6-a6bb-58e5313b1cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "import spacy\n",
        "from spacy_ngram import NgramComponent\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')  # or whatever model you downloaded\n",
        "nlp.add_pipe('spacy-ngram')  # default to document-level ngrams, removing stopwords\n",
        "\n",
        "text = 'Quark soup is an interacting localized assembly of quarks and gluons.'\n",
        "print(nlp(text)._.ngram_1)\n",
        "print(nlp(text)._.ngram_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUDCGNdvKUPe",
        "outputId": "bbb8a08c-63b5-4526-9993-bada9917f1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quark', 'soup', 'interact', 'localize', 'assembly', 'quark', 'gluon']\n",
            "['quark_soup', 'soup_interact', 'interact_localize', 'localize_assembly', 'assembly_quark', 'quark_gluon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using NLTK\n",
        "from nltk.util import ngrams\n",
        "text = 'Quark soup is an interacting localized assembly of quarks and gluons.'\n",
        "\n",
        "unigrams = ngrams(text.split(), 1)\n",
        "for item in unigrams:\n",
        "    print(item)\n",
        "bigrams = ngrams(text.split(), 2)\n",
        "for item in bigrams:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSo5Hdr4MruT",
        "outputId": "3e4a804e-01e6-4404-8793-d1841b68d279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Quark',)\n",
            "('soup',)\n",
            "('is',)\n",
            "('an',)\n",
            "('interacting',)\n",
            "('localized',)\n",
            "('assembly',)\n",
            "('of',)\n",
            "('quarks',)\n",
            "('and',)\n",
            "('gluons.',)\n",
            "('Quark', 'soup')\n",
            "('soup', 'is')\n",
            "('is', 'an')\n",
            "('an', 'interacting')\n",
            "('interacting', 'localized')\n",
            "('localized', 'assembly')\n",
            "('assembly', 'of')\n",
            "('of', 'quarks')\n",
            "('quarks', 'and')\n",
            "('and', 'gluons.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Lemmas and Stems**"
      ],
      "metadata": {
        "id": "xLfez0c-N2dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization Using Spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u\"he was running late\")\n",
        "for token in doc:\n",
        "  print('{} > {}'.format(token, token.lemma_))\n",
        "#There is no stemming method in Spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLQL4S4KNjvw",
        "outputId": "3e08da9b-c762-400e-afdf-2bf4f47444c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he > he\n",
            "was > be\n",
            "running > run\n",
            "late > late\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization using NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn=WordNetLemmatizer()\n",
        "doc =\"he was running late\"\n",
        "[wn.lemmatize(word) for word in doc.split()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juM4FwChQ4oO",
        "outputId": "99e838ee-0d0c-4faf-81a0-d9b85fded7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'wa', 'running', 'late']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization using NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "doc =\"he was running late\"\n",
        "[ps.stem(word) for word in doc.split()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDeCErGc2oB",
        "outputId": "f99f6b59-bec3-4843-da3e-8938c50a2fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'wa', 'run', 'late']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Categorizing words: POS Tagging**"
      ],
      "metadata": {
        "id": "23ue5Rfid0Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for token in doc:\n",
        "  print('{} {}'.format(token, token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "265PPL3udB42",
        "outputId": "9242d840-00db-49b0-8197-a7db78530159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary PROPN\n",
            "slapped VERB\n",
            "the DET\n",
            "green ADJ\n",
            "witch NOUN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using NLTK\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "doc = \"Mary slapped the green witch.\"\n",
        "pos_tag(word_tokenize(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FD_-Oohd7FW",
        "outputId": "672258eb-71b6-4086-f1d0-bdc3597d74c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mary', 'NNP'),\n",
              " ('slapped', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('green', 'JJ'),\n",
              " ('witch', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Categorizing Spans: Chunking and Named Entity Recognition**"
      ],
      "metadata": {
        "id": "Ugyw8CnwCXhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking:** Often, we need to label a span of text; that is, a contiguous multitoken boundary. For example,\n",
        "consider the sentence, “Mary slapped the green witch.” We might want to identify the noun phrases\n",
        "(NP) and verb phrases (VP) in it,"
      ],
      "metadata": {
        "id": "7DSC7Z3cCdXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u\"Mary slapped the green witch.\")\n",
        "for chunk in doc.noun_chunks:\n",
        "  print ('{} {}'.format(chunk, chunk.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w44wccBCcJa",
        "outputId": "23230e14-7e10-49ed-acb9-acbd4561d0cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary NP\n",
            "the green witch NP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity**:  A named entity is a string mention of a real\n",
        "world concept like a person, location, organization, drug name, and so on."
      ],
      "metadata": {
        "id": "5kXoOrd-EnuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using spacy\n",
        "doc = nlp(\"Larry Page founded Google\")\n",
        "# Text and label of named entity span\n",
        "[(ent.text, ent.label_) for ent in doc.ents]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztSPuF3_C7_w",
        "outputId": "0e90c162-110b-4a6b-84a5-4461d41a5480"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Larry Page', 'PERSON'), ('Google', 'ORG')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using NLTK\n",
        "import nltk\n",
        "from nltk import word_tokenize,pos_tag\n",
        "from nltk import ne_chunk\n",
        "text = \"NASA awarded Elon Musk’s SpaceX a $2.9 billion contract to build the lunar lander.\"\n",
        "tokens = word_tokenize(text)\n",
        "tag=pos_tag(tokens)\n",
        "print(tag)\n",
        "\n",
        "ne_tree = nltk.ne_chunk(tag)\n",
        "print(ne_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeuX5rQqEjdd",
        "outputId": "8da2c5a2-044a-44ca-a427-6925d26a4bd8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NASA', 'NNP'), ('awarded', 'VBD'), ('Elon', 'NNP'), ('Musk', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('SpaceX', 'NNP'), ('a', 'DT'), ('$', '$'), ('2.9', 'CD'), ('billion', 'CD'), ('contract', 'NN'), ('to', 'TO'), ('build', 'VB'), ('the', 'DT'), ('lunar', 'NN'), ('lander', 'NN'), ('.', '.')]\n",
            "(S\n",
            "  (ORGANIZATION NASA/NNP)\n",
            "  awarded/VBD\n",
            "  (PERSON Elon/NNP Musk/NNP)\n",
            "  ’/NNP\n",
            "  s/VBD\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  a/DT\n",
            "  $/$\n",
            "  2.9/CD\n",
            "  billion/CD\n",
            "  contract/NN\n",
            "  to/TO\n",
            "  build/VB\n",
            "  the/DT\n",
            "  lunar/NN\n",
            "  lander/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}